{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have learnt now how to tokenize strings, using various methods. \n",
      "In this session you will learn the following things:\n",
      "\n",
      "* Learn how to convert text into vectors (examples using scikit-learn)\n",
      "* Learn how to extract information from text (a.k.a information extraction)\n",
      "\n",
      "and some additional concepts as we go (bonus!!!)\n",
      "\n",
      "So lets start with a corpus that Dave has put together.\n",
      "\n",
      "Fetch the data. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "deal_corpus = requests.get('https://raw.github.com/dlemphers-rmn/mlclass/master/deals.txt').text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "deal_corpus = deal_corpus.split('\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So lets check how many deal titles we have in our corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(deal_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For any Machine Learning to be performed on textual data specially it is desirable to convert it into numeric vectors. \n",
      "Now how can we convert text data into numbers? \n",
      "\n",
      "* Generating a list of all the unique words in the corpus and then checking presence of each of the words in the corpus\n",
      "\n",
      "This is the NLP jargon is known as [Bag of Words or BOW approach](http://en.wikipedia.org/wiki/Bag-of-words_model)\n",
      "\n",
      "For example:\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "example_documents = ('Online Fingerstyle Guitar Lessons',\n",
      "'Online Country Guitar Lessons',\n",
      "'Clean Up Your Credit Report',\n",
      "'Learn Double Bass Online',\n",
      "'Folica 25% Off Sitewide')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.feature_extraction.text\n",
      "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print count_vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count_vectorizer.fit_transform(example_documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pprint  \n",
      "pprint.pprint(count_vectorizer.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Convert to a vector matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vector_matrix = count_vectorizer.transform(example_documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vector_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vector_matrix.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vanilla count vectorizers are good but they are not powerful enough but there are following issues:\n",
      "\n",
      "* it scales up frequent terms and scales down rare terms which are empirically more informative than the high frequency terms.\n",
      "* we want to emphasize on un(common) terms as a means of describing each vector in a powerful way\n",
      "\n",
      "Solution\n",
      "========\n",
      "\n",
      "Term-frequency inverse document frequency ([tf\u2013idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf))\n",
      "\n",
      "What tf-idf gives is how important is a word to a document in a collection, and that\u2019s why tf-idf incorporates local and global parameters, because it takes in consideration not only the isolated term but also the term within the document collection. What tf-idf then does to solve that problem, is to scale down the frequent terms while scaling up the rare terms; a term that occurs 10 times more than another isn\u2019t 10 times more important than it, that\u2019s why tf-idf uses the logarithmic scale to do that."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![TFIDF Explained](http://archimedes.fas.harvard.edu/presentations/2002-03-09/img13.gif)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tfidf_vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TfidfVectorizer(analyzer=word, binary=False, charset=None, charset_error=None,\n",
        "        decode_error=strict, dtype=<type 'numpy.int64'>, encoding=utf-8,\n",
        "        input=content, lowercase=True, max_df=1.0, max_features=None,\n",
        "        min_df=1, ngram_range=(1, 1), norm=l2, preprocessor=None,\n",
        "        smooth_idf=True, stop_words=None, strip_accents=None,\n",
        "        sublinear_tf=False, token_pattern=(?u)\\b\\w\\w+\\b, tokenizer=None,\n",
        "        use_idf=True, vocabulary=None)\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Refer to [scikit-learn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) on tfidf vectorizer for more details."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_vectorizer.fit_transform(example_documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(tfidf_vectorizer.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidfvector_matrix = tfidf_vectorizer.transform(example_documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tfidfvector_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(tfidfvector_matrix.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(tfidf_vectorizer.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some tricks\n",
      "===========\n",
      "\n",
      "* we can use n-grams when trying to do tfidf transformation\n",
      "\n",
      "ngrams are explained by Dave in his session"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngram_tfidf = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1, 4))\n",
      "ngram_tfidf_matrix = ngram_tfidf.fit_transform(example_documents)\n",
      "pprint.pprint(ngram_tfidf.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'25': 0,\n",
        " u'25 off': 1,\n",
        " u'25 off sitewide': 2,\n",
        " u'bass': 3,\n",
        " u'bass online': 4,\n",
        " u'clean': 5,\n",
        " u'clean up': 6,\n",
        " u'clean up your': 7,\n",
        " u'clean up your credit': 8,\n",
        " u'country': 9,\n",
        " u'country guitar': 10,\n",
        " u'country guitar lessons': 11,\n",
        " u'credit': 12,\n",
        " u'credit report': 13,\n",
        " u'double': 14,\n",
        " u'double bass': 15,\n",
        " u'double bass online': 16,\n",
        " u'fingerstyle': 17,\n",
        " u'fingerstyle guitar': 18,\n",
        " u'fingerstyle guitar lessons': 19,\n",
        " u'folica': 20,\n",
        " u'folica 25': 21,\n",
        " u'folica 25 off': 22,\n",
        " u'folica 25 off sitewide': 23,\n",
        " u'guitar': 24,\n",
        " u'guitar lessons': 25,\n",
        " u'learn': 26,\n",
        " u'learn double': 27,\n",
        " u'learn double bass': 28,\n",
        " u'learn double bass online': 29,\n",
        " u'lessons': 30,\n",
        " u'off': 31,\n",
        " u'off sitewide': 32,\n",
        " u'online': 33,\n",
        " u'online country': 34,\n",
        " u'online country guitar': 35,\n",
        " u'online country guitar lessons': 36,\n",
        " u'online fingerstyle': 37,\n",
        " u'online fingerstyle guitar': 38,\n",
        " u'online fingerstyle guitar lessons': 39,\n",
        " u'report': 40,\n",
        " u'sitewide': 41,\n",
        " u'up': 42,\n",
        " u'up your': 43,\n",
        " u'up your credit': 44,\n",
        " u'up your credit report': 45,\n",
        " u'your': 46,\n",
        " u'your credit': 47,\n",
        " u'your credit report': 48}\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* we can remove stop words\n",
      "\n",
      "Basically stopwords are very-very common words in the english language"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.corpus\n",
      "stopwords_english = nltk.corpus.stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(stopwords_english[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For corpus which is typically short we can use character n-grams which are at a character level.\n",
      "\n",
      "This will result in a dense vector representation but helps to extract maximum juice out of the short corpus.\n",
      "\n",
      "*extra: you can use a different tokenizer (ref: Dave's sessions)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngram_tfidf = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1, 7),analyzer='char')\n",
      "ngram_tfidf_matrix = ngram_tfidf.fit_transform(example_documents)\n",
      "pprint.pprint(ngram_tfidf.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u' ': 0,\n",
        " u' 2': 1,\n",
        " u' 25': 2,\n",
        " u' 25%': 3,\n",
        " u' 25% ': 4,\n",
        " u' 25% o': 5,\n",
        " u' 25% of': 6,\n",
        " u' b': 7,\n",
        " u' ba': 8,\n",
        " u' bas': 9,\n",
        " u' bass': 10,\n",
        " u' bass ': 11,\n",
        " u' bass o': 12,\n",
        " u' c': 13,\n",
        " u' co': 14,\n",
        " u' cou': 15,\n",
        " u' coun': 16,\n",
        " u' count': 17,\n",
        " u' countr': 18,\n",
        " u' cr': 19,\n",
        " u' cre': 20,\n",
        " u' cred': 21,\n",
        " u' credi': 22,\n",
        " u' credit': 23,\n",
        " u' d': 24,\n",
        " u' do': 25,\n",
        " u' dou': 26,\n",
        " u' doub': 27,\n",
        " u' doubl': 28,\n",
        " u' double': 29,\n",
        " u' f': 30,\n",
        " u' fi': 31,\n",
        " u' fin': 32,\n",
        " u' fing': 33,\n",
        " u' finge': 34,\n",
        " u' finger': 35,\n",
        " u' g': 36,\n",
        " u' gu': 37,\n",
        " u' gui': 38,\n",
        " u' guit': 39,\n",
        " u' guita': 40,\n",
        " u' guitar': 41,\n",
        " u' l': 42,\n",
        " u' le': 43,\n",
        " u' les': 44,\n",
        " u' less': 45,\n",
        " u' lesso': 46,\n",
        " u' lesson': 47,\n",
        " u' o': 48,\n",
        " u' of': 49,\n",
        " u' off': 50,\n",
        " u' off ': 51,\n",
        " u' off s': 52,\n",
        " u' off si': 53,\n",
        " u' on': 54,\n",
        " u' onl': 55,\n",
        " u' onli': 56,\n",
        " u' onlin': 57,\n",
        " u' online': 58,\n",
        " u' r': 59,\n",
        " u' re': 60,\n",
        " u' rep': 61,\n",
        " u' repo': 62,\n",
        " u' repor': 63,\n",
        " u' report': 64,\n",
        " u' s': 65,\n",
        " u' si': 66,\n",
        " u' sit': 67,\n",
        " u' site': 68,\n",
        " u' sitew': 69,\n",
        " u' sitewi': 70,\n",
        " u' u': 71,\n",
        " u' up': 72,\n",
        " u' up ': 73,\n",
        " u' up y': 74,\n",
        " u' up yo': 75,\n",
        " u' up you': 76,\n",
        " u' y': 77,\n",
        " u' yo': 78,\n",
        " u' you': 79,\n",
        " u' your': 80,\n",
        " u' your ': 81,\n",
        " u' your c': 82,\n",
        " u'%': 83,\n",
        " u'% ': 84,\n",
        " u'% o': 85,\n",
        " u'% of': 86,\n",
        " u'% off': 87,\n",
        " u'% off ': 88,\n",
        " u'% off s': 89,\n",
        " u'2': 90,\n",
        " u'25': 91,\n",
        " u'25%': 92,\n",
        " u'25% ': 93,\n",
        " u'25% o': 94,\n",
        " u'25% of': 95,\n",
        " u'25% off': 96,\n",
        " u'5': 97,\n",
        " u'5%': 98,\n",
        " u'5% ': 99,\n",
        " u'5% o': 100,\n",
        " u'5% of': 101,\n",
        " u'5% off': 102,\n",
        " u'5% off ': 103,\n",
        " u'a': 104,\n",
        " u'a ': 105,\n",
        " u'a 2': 106,\n",
        " u'a 25': 107,\n",
        " u'a 25%': 108,\n",
        " u'a 25% ': 109,\n",
        " u'a 25% o': 110,\n",
        " u'an': 111,\n",
        " u'an ': 112,\n",
        " u'an u': 113,\n",
        " u'an up': 114,\n",
        " u'an up ': 115,\n",
        " u'an up y': 116,\n",
        " u'ar': 117,\n",
        " u'ar ': 118,\n",
        " u'ar l': 119,\n",
        " u'ar le': 120,\n",
        " u'ar les': 121,\n",
        " u'ar less': 122,\n",
        " u'arn': 123,\n",
        " u'arn ': 124,\n",
        " u'arn d': 125,\n",
        " u'arn do': 126,\n",
        " u'arn dou': 127,\n",
        " u'as': 128,\n",
        " u'ass': 129,\n",
        " u'ass ': 130,\n",
        " u'ass o': 131,\n",
        " u'ass on': 132,\n",
        " u'ass onl': 133,\n",
        " u'b': 134,\n",
        " u'ba': 135,\n",
        " u'bas': 136,\n",
        " u'bass': 137,\n",
        " u'bass ': 138,\n",
        " u'bass o': 139,\n",
        " u'bass on': 140,\n",
        " u'bl': 141,\n",
        " u'ble': 142,\n",
        " u'ble ': 143,\n",
        " u'ble b': 144,\n",
        " u'ble ba': 145,\n",
        " u'ble bas': 146,\n",
        " u'c': 147,\n",
        " u'ca': 148,\n",
        " u'ca ': 149,\n",
        " u'ca 2': 150,\n",
        " u'ca 25': 151,\n",
        " u'ca 25%': 152,\n",
        " u'ca 25% ': 153,\n",
        " u'cl': 154,\n",
        " u'cle': 155,\n",
        " u'clea': 156,\n",
        " u'clean': 157,\n",
        " u'clean ': 158,\n",
        " u'clean u': 159,\n",
        " u'co': 160,\n",
        " u'cou': 161,\n",
        " u'coun': 162,\n",
        " u'count': 163,\n",
        " u'countr': 164,\n",
        " u'country': 165,\n",
        " u'cr': 166,\n",
        " u'cre': 167,\n",
        " u'cred': 168,\n",
        " u'credi': 169,\n",
        " u'credit': 170,\n",
        " u'credit ': 171,\n",
        " u'd': 172,\n",
        " u'de': 173,\n",
        " u'di': 174,\n",
        " u'dit': 175,\n",
        " u'dit ': 176,\n",
        " u'dit r': 177,\n",
        " u'dit re': 178,\n",
        " u'dit rep': 179,\n",
        " u'do': 180,\n",
        " u'dou': 181,\n",
        " u'doub': 182,\n",
        " u'doubl': 183,\n",
        " u'double': 184,\n",
        " u'double ': 185,\n",
        " u'e': 186,\n",
        " u'e ': 187,\n",
        " u'e b': 188,\n",
        " u'e ba': 189,\n",
        " u'e bas': 190,\n",
        " u'e bass': 191,\n",
        " u'e bass ': 192,\n",
        " u'e c': 193,\n",
        " u'e co': 194,\n",
        " u'e cou': 195,\n",
        " u'e coun': 196,\n",
        " u'e count': 197,\n",
        " u'e f': 198,\n",
        " u'e fi': 199,\n",
        " u'e fin': 200,\n",
        " u'e fing': 201,\n",
        " u'e finge': 202,\n",
        " u'e g': 203,\n",
        " u'e gu': 204,\n",
        " u'e gui': 205,\n",
        " u'e guit': 206,\n",
        " u'e guita': 207,\n",
        " u'ea': 208,\n",
        " u'ean': 209,\n",
        " u'ean ': 210,\n",
        " u'ean u': 211,\n",
        " u'ean up': 212,\n",
        " u'ean up ': 213,\n",
        " u'ear': 214,\n",
        " u'earn': 215,\n",
        " u'earn ': 216,\n",
        " u'earn d': 217,\n",
        " u'earn do': 218,\n",
        " u'ed': 219,\n",
        " u'edi': 220,\n",
        " u'edit': 221,\n",
        " u'edit ': 222,\n",
        " u'edit r': 223,\n",
        " u'edit re': 224,\n",
        " u'ep': 225,\n",
        " u'epo': 226,\n",
        " u'epor': 227,\n",
        " u'eport': 228,\n",
        " u'er': 229,\n",
        " u'ers': 230,\n",
        " u'erst': 231,\n",
        " u'ersty': 232,\n",
        " u'erstyl': 233,\n",
        " u'erstyle': 234,\n",
        " u'es': 235,\n",
        " u'ess': 236,\n",
        " u'esso': 237,\n",
        " u'esson': 238,\n",
        " u'essons': 239,\n",
        " u'ew': 240,\n",
        " u'ewi': 241,\n",
        " u'ewid': 242,\n",
        " u'ewide': 243,\n",
        " u'f': 244,\n",
        " u'f ': 245,\n",
        " u'f s': 246,\n",
        " u'f si': 247,\n",
        " u'f sit': 248,\n",
        " u'f site': 249,\n",
        " u'f sitew': 250,\n",
        " u'ff': 251,\n",
        " u'ff ': 252,\n",
        " u'ff s': 253,\n",
        " u'ff si': 254,\n",
        " u'ff sit': 255,\n",
        " u'ff site': 256,\n",
        " u'fi': 257,\n",
        " u'fin': 258,\n",
        " u'fing': 259,\n",
        " u'finge': 260,\n",
        " u'finger': 261,\n",
        " u'fingers': 262,\n",
        " u'fo': 263,\n",
        " u'fol': 264,\n",
        " u'foli': 265,\n",
        " u'folic': 266,\n",
        " u'folica': 267,\n",
        " u'folica ': 268,\n",
        " u'g': 269,\n",
        " u'ge': 270,\n",
        " u'ger': 271,\n",
        " u'gers': 272,\n",
        " u'gerst': 273,\n",
        " u'gersty': 274,\n",
        " u'gerstyl': 275,\n",
        " u'gu': 276,\n",
        " u'gui': 277,\n",
        " u'guit': 278,\n",
        " u'guita': 279,\n",
        " u'guitar': 280,\n",
        " u'guitar ': 281,\n",
        " u'i': 282,\n",
        " u'ic': 283,\n",
        " u'ica': 284,\n",
        " u'ica ': 285,\n",
        " u'ica 2': 286,\n",
        " u'ica 25': 287,\n",
        " u'ica 25%': 288,\n",
        " u'id': 289,\n",
        " u'ide': 290,\n",
        " u'in': 291,\n",
        " u'ine': 292,\n",
        " u'ine ': 293,\n",
        " u'ine c': 294,\n",
        " u'ine co': 295,\n",
        " u'ine cou': 296,\n",
        " u'ine f': 297,\n",
        " u'ine fi': 298,\n",
        " u'ine fin': 299,\n",
        " u'ing': 300,\n",
        " u'inge': 301,\n",
        " u'inger': 302,\n",
        " u'ingers': 303,\n",
        " u'ingerst': 304,\n",
        " u'it': 305,\n",
        " u'it ': 306,\n",
        " u'it r': 307,\n",
        " u'it re': 308,\n",
        " u'it rep': 309,\n",
        " u'it repo': 310,\n",
        " u'ita': 311,\n",
        " u'itar': 312,\n",
        " u'itar ': 313,\n",
        " u'itar l': 314,\n",
        " u'itar le': 315,\n",
        " u'ite': 316,\n",
        " u'itew': 317,\n",
        " u'itewi': 318,\n",
        " u'itewid': 319,\n",
        " u'itewide': 320,\n",
        " u'l': 321,\n",
        " u'le': 322,\n",
        " u'le ': 323,\n",
        " u'le b': 324,\n",
        " u'le ba': 325,\n",
        " u'le bas': 326,\n",
        " u'le bass': 327,\n",
        " u'le g': 328,\n",
        " u'le gu': 329,\n",
        " u'le gui': 330,\n",
        " u'le guit': 331,\n",
        " u'lea': 332,\n",
        " u'lean': 333,\n",
        " u'lean ': 334,\n",
        " u'lean u': 335,\n",
        " u'lean up': 336,\n",
        " u'lear': 337,\n",
        " u'learn': 338,\n",
        " u'learn ': 339,\n",
        " u'learn d': 340,\n",
        " u'les': 341,\n",
        " u'less': 342,\n",
        " u'lesso': 343,\n",
        " u'lesson': 344,\n",
        " u'lessons': 345,\n",
        " u'li': 346,\n",
        " u'lic': 347,\n",
        " u'lica': 348,\n",
        " u'lica ': 349,\n",
        " u'lica 2': 350,\n",
        " u'lica 25': 351,\n",
        " u'lin': 352,\n",
        " u'line': 353,\n",
        " u'line ': 354,\n",
        " u'line c': 355,\n",
        " u'line co': 356,\n",
        " u'line f': 357,\n",
        " u'line fi': 358,\n",
        " u'n': 359,\n",
        " u'n ': 360,\n",
        " u'n d': 361,\n",
        " u'n do': 362,\n",
        " u'n dou': 363,\n",
        " u'n doub': 364,\n",
        " u'n doubl': 365,\n",
        " u'n u': 366,\n",
        " u'n up': 367,\n",
        " u'n up ': 368,\n",
        " u'n up y': 369,\n",
        " u'n up yo': 370,\n",
        " u'ne': 371,\n",
        " u'ne ': 372,\n",
        " u'ne c': 373,\n",
        " u'ne co': 374,\n",
        " u'ne cou': 375,\n",
        " u'ne coun': 376,\n",
        " u'ne f': 377,\n",
        " u'ne fi': 378,\n",
        " u'ne fin': 379,\n",
        " u'ne fing': 380,\n",
        " u'ng': 381,\n",
        " u'nge': 382,\n",
        " u'nger': 383,\n",
        " u'ngers': 384,\n",
        " u'ngerst': 385,\n",
        " u'ngersty': 386,\n",
        " u'nl': 387,\n",
        " u'nli': 388,\n",
        " u'nlin': 389,\n",
        " u'nline': 390,\n",
        " u'nline ': 391,\n",
        " u'nline c': 392,\n",
        " u'nline f': 393,\n",
        " u'ns': 394,\n",
        " u'nt': 395,\n",
        " u'ntr': 396,\n",
        " u'ntry': 397,\n",
        " u'ntry ': 398,\n",
        " u'ntry g': 399,\n",
        " u'ntry gu': 400,\n",
        " u'o': 401,\n",
        " u'of': 402,\n",
        " u'off': 403,\n",
        " u'off ': 404,\n",
        " u'off s': 405,\n",
        " u'off si': 406,\n",
        " u'off sit': 407,\n",
        " u'ol': 408,\n",
        " u'oli': 409,\n",
        " u'olic': 410,\n",
        " u'olica': 411,\n",
        " u'olica ': 412,\n",
        " u'olica 2': 413,\n",
        " u'on': 414,\n",
        " u'onl': 415,\n",
        " u'onli': 416,\n",
        " u'onlin': 417,\n",
        " u'online': 418,\n",
        " u'online ': 419,\n",
        " u'ons': 420,\n",
        " u'or': 421,\n",
        " u'ort': 422,\n",
        " u'ou': 423,\n",
        " u'oub': 424,\n",
        " u'oubl': 425,\n",
        " u'ouble': 426,\n",
        " u'ouble ': 427,\n",
        " u'ouble b': 428,\n",
        " u'oun': 429,\n",
        " u'ount': 430,\n",
        " u'ountr': 431,\n",
        " u'ountry': 432,\n",
        " u'ountry ': 433,\n",
        " u'our': 434,\n",
        " u'our ': 435,\n",
        " u'our c': 436,\n",
        " u'our cr': 437,\n",
        " u'our cre': 438,\n",
        " u'p': 439,\n",
        " u'p ': 440,\n",
        " u'p y': 441,\n",
        " u'p yo': 442,\n",
        " u'p you': 443,\n",
        " u'p your': 444,\n",
        " u'p your ': 445,\n",
        " u'po': 446,\n",
        " u'por': 447,\n",
        " u'port': 448,\n",
        " u'r': 449,\n",
        " u'r ': 450,\n",
        " u'r c': 451,\n",
        " u'r cr': 452,\n",
        " u'r cre': 453,\n",
        " u'r cred': 454,\n",
        " u'r credi': 455,\n",
        " u'r l': 456,\n",
        " u'r le': 457,\n",
        " u'r les': 458,\n",
        " u'r less': 459,\n",
        " u'r lesso': 460,\n",
        " u're': 461,\n",
        " u'red': 462,\n",
        " u'redi': 463,\n",
        " u'redit': 464,\n",
        " u'redit ': 465,\n",
        " u'redit r': 466,\n",
        " u'rep': 467,\n",
        " u'repo': 468,\n",
        " u'repor': 469,\n",
        " u'report': 470,\n",
        " u'rn': 471,\n",
        " u'rn ': 472,\n",
        " u'rn d': 473,\n",
        " u'rn do': 474,\n",
        " u'rn dou': 475,\n",
        " u'rn doub': 476,\n",
        " u'rs': 477,\n",
        " u'rst': 478,\n",
        " u'rsty': 479,\n",
        " u'rstyl': 480,\n",
        " u'rstyle': 481,\n",
        " u'rstyle ': 482,\n",
        " u'rt': 483,\n",
        " u'ry': 484,\n",
        " u'ry ': 485,\n",
        " u'ry g': 486,\n",
        " u'ry gu': 487,\n",
        " u'ry gui': 488,\n",
        " u'ry guit': 489,\n",
        " u's': 490,\n",
        " u's ': 491,\n",
        " u's o': 492,\n",
        " u's on': 493,\n",
        " u's onl': 494,\n",
        " u's onli': 495,\n",
        " u's onlin': 496,\n",
        " u'si': 497,\n",
        " u'sit': 498,\n",
        " u'site': 499,\n",
        " u'sitew': 500,\n",
        " u'sitewi': 501,\n",
        " u'sitewid': 502,\n",
        " u'so': 503,\n",
        " u'son': 504,\n",
        " u'sons': 505,\n",
        " u'ss': 506,\n",
        " u'ss ': 507,\n",
        " u'ss o': 508,\n",
        " u'ss on': 509,\n",
        " u'ss onl': 510,\n",
        " u'ss onli': 511,\n",
        " u'sso': 512,\n",
        " u'sson': 513,\n",
        " u'ssons': 514,\n",
        " u'st': 515,\n",
        " u'sty': 516,\n",
        " u'styl': 517,\n",
        " u'style': 518,\n",
        " u'style ': 519,\n",
        " u'style g': 520,\n",
        " u't': 521,\n",
        " u't ': 522,\n",
        " u't r': 523,\n",
        " u't re': 524,\n",
        " u't rep': 525,\n",
        " u't repo': 526,\n",
        " u't repor': 527,\n",
        " u'ta': 528,\n",
        " u'tar': 529,\n",
        " u'tar ': 530,\n",
        " u'tar l': 531,\n",
        " u'tar le': 532,\n",
        " u'tar les': 533,\n",
        " u'te': 534,\n",
        " u'tew': 535,\n",
        " u'tewi': 536,\n",
        " u'tewid': 537,\n",
        " u'tewide': 538,\n",
        " u'tr': 539,\n",
        " u'try': 540,\n",
        " u'try ': 541,\n",
        " u'try g': 542,\n",
        " u'try gu': 543,\n",
        " u'try gui': 544,\n",
        " u'ty': 545,\n",
        " u'tyl': 546,\n",
        " u'tyle': 547,\n",
        " u'tyle ': 548,\n",
        " u'tyle g': 549,\n",
        " u'tyle gu': 550,\n",
        " u'u': 551,\n",
        " u'ub': 552,\n",
        " u'ubl': 553,\n",
        " u'uble': 554,\n",
        " u'uble ': 555,\n",
        " u'uble b': 556,\n",
        " u'uble ba': 557,\n",
        " u'ui': 558,\n",
        " u'uit': 559,\n",
        " u'uita': 560,\n",
        " u'uitar': 561,\n",
        " u'uitar ': 562,\n",
        " u'uitar l': 563,\n",
        " u'un': 564,\n",
        " u'unt': 565,\n",
        " u'untr': 566,\n",
        " u'untry': 567,\n",
        " u'untry ': 568,\n",
        " u'untry g': 569,\n",
        " u'up': 570,\n",
        " u'up ': 571,\n",
        " u'up y': 572,\n",
        " u'up yo': 573,\n",
        " u'up you': 574,\n",
        " u'up your': 575,\n",
        " u'ur': 576,\n",
        " u'ur ': 577,\n",
        " u'ur c': 578,\n",
        " u'ur cr': 579,\n",
        " u'ur cre': 580,\n",
        " u'ur cred': 581,\n",
        " u'w': 582,\n",
        " u'wi': 583,\n",
        " u'wid': 584,\n",
        " u'wide': 585,\n",
        " u'y': 586,\n",
        " u'y ': 587,\n",
        " u'y g': 588,\n",
        " u'y gu': 589,\n",
        " u'y gui': 590,\n",
        " u'y guit': 591,\n",
        " u'y guita': 592,\n",
        " u'yl': 593,\n",
        " u'yle': 594,\n",
        " u'yle ': 595,\n",
        " u'yle g': 596,\n",
        " u'yle gu': 597,\n",
        " u'yle gui': 598,\n",
        " u'yo': 599,\n",
        " u'you': 600,\n",
        " u'your': 601,\n",
        " u'your ': 602,\n",
        " u'your c': 603,\n",
        " u'your cr': 604}\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngram_tfidf_deal_corpus = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1, 7),analyzer='char',stop_words=stopwords_english)\n",
      "ngram_tfidf_matrix_deal_corpus = ngram_tfidf.fit_transform(example_documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(ngram_tfidf_matrix_deal_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<5x605 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 773 stored elements in Compressed Sparse Row format>\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These vector matrices can be used as datapoints to perform: \n",
      "\n",
      "- clustering (unsupervised learning)\n",
      "- classification (supervised learning)\n",
      "\n",
      "for more details visit this example from [scikit-learn - classification](http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html) and [scikit-learn - clustering](http://scikit-learn.org/stable/auto_examples/document_clustering.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "End\n",
      "==="
     ]
    }
   ],
   "metadata": {}
  }
 ]
}