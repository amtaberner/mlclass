{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why information extraction from natural language?\n",
      "=================================================\n",
      "\n",
      "Natural language is unstructured and it is important to extract information from text.\n",
      "\n",
      "- example extracting phone numbers, adresses and email address from web pages.\n",
      "\n",
      "For us it is very lucrative as deal titles and descriptions contain valuable information like:\n",
      "\n",
      "* Deal (free shipping, get X% off)\n",
      "* Product\n",
      "* Start and End dates\n",
      "* Restrictions\n",
      "\n",
      "So let us learn some basics about extraction of information from free text. \n",
      "\n",
      "Following is a high level overview of what the workflow of information extraction will look like.\n",
      "\n",
      "![Information Extraction](http://nltk.org/images/ie-architecture.png)\n",
      "\n",
      "We will not touch relation extraction as it is advanced topic. \n",
      "\n",
      "Also I can show you some of the deal specific extractors we have put together but in the class I will focus on general examples as we are running on a tight schedule."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import re\n",
      "import pprint\n",
      "document = 'Save 20% off on regular, sale and clearance prices, including select fine & fashion jewelry + 15% on home, select watches, shoes, coats, suits, dresses, lingerie, men?s suit separates & sport coats. Plus free shipping on orders over $99. Exclusions apply. Expires on 12/19/2013.'\n",
      "sentences = nltk.sent_tokenize(document)\n",
      "# we are performing sentence segmentation here\n",
      "pprint.pprint(sentences)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Save 20% off on regular, sale and clearance prices, including select fine & fashion jewelry + 15% on home, select watches, shoes, coats, suits, dresses, lingerie, men?s suit separates & sport coats.',\n",
        " 'Plus free shipping on orders over $99.',\n",
        " 'Exclusions apply.',\n",
        " 'Expires on 12/19/2013.']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# next is tokenization\n",
      "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
      "pprint.pprint(sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['Save',\n",
        "  '20',\n",
        "  '%',\n",
        "  'off',\n",
        "  'on',\n",
        "  'regular',\n",
        "  ',',\n",
        "  'sale',\n",
        "  'and',\n",
        "  'clearance',\n",
        "  'prices',\n",
        "  ',',\n",
        "  'including',\n",
        "  'select',\n",
        "  'fine',\n",
        "  '&',\n",
        "  'fashion',\n",
        "  'jewelry',\n",
        "  '+',\n",
        "  '15',\n",
        "  '%',\n",
        "  'on',\n",
        "  'home',\n",
        "  ',',\n",
        "  'select',\n",
        "  'watches',\n",
        "  ',',\n",
        "  'shoes',\n",
        "  ',',\n",
        "  'coats',\n",
        "  ',',\n",
        "  'suits',\n",
        "  ',',\n",
        "  'dresses',\n",
        "  ',',\n",
        "  'lingerie',\n",
        "  ',',\n",
        "  'men',\n",
        "  '?',\n",
        "  's',\n",
        "  'suit',\n",
        "  'separates',\n",
        "  '&',\n",
        "  'sport',\n",
        "  'coats',\n",
        "  '.'],\n",
        " ['Plus', 'free', 'shipping', 'on', 'orders', 'over', '$', '99', '.'],\n",
        " ['Exclusions', 'apply', '.'],\n",
        " ['Expires', 'on', '12/19/2013', '.']]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# next perform part of speech tagging\n",
      "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
      "pprint.pprint(sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[('Save', 'NNP'),\n",
        "  ('20', 'CD'),\n",
        "  ('%', 'NN'),\n",
        "  ('off', 'IN'),\n",
        "  ('on', 'IN'),\n",
        "  ('regular', 'JJ'),\n",
        "  (',', ','),\n",
        "  ('sale', 'NN'),\n",
        "  ('and', 'CC'),\n",
        "  ('clearance', 'NN'),\n",
        "  ('prices', 'NNS'),\n",
        "  (',', ','),\n",
        "  ('including', 'VBG'),\n",
        "  ('select', 'NN'),\n",
        "  ('fine', 'NN'),\n",
        "  ('&', 'CC'),\n",
        "  ('fashion', 'NN'),\n",
        "  ('jewelry', 'NN'),\n",
        "  ('+', ':'),\n",
        "  ('15', 'CD'),\n",
        "  ('%', 'NN'),\n",
        "  ('on', 'IN'),\n",
        "  ('home', 'NN'),\n",
        "  (',', ','),\n",
        "  ('select', 'NN'),\n",
        "  ('watches', 'NNS'),\n",
        "  (',', ','),\n",
        "  ('shoes', 'VBZ'),\n",
        "  (',', ','),\n",
        "  ('coats', 'NNS'),\n",
        "  (',', ','),\n",
        "  ('suits', 'NNS'),\n",
        "  (',', ','),\n",
        "  ('dresses', 'NNS'),\n",
        "  (',', ','),\n",
        "  ('lingerie', 'VBP'),\n",
        "  (',', ','),\n",
        "  ('men', 'NNS'),\n",
        "  ('?', '.'),\n",
        "  ('s', 'NNS'),\n",
        "  ('suit', 'NN'),\n",
        "  ('separates', 'NNS'),\n",
        "  ('&', 'CC'),\n",
        "  ('sport', 'NN'),\n",
        "  ('coats', 'NNS'),\n",
        "  ('.', '.')],\n",
        " [('Plus', 'NNP'),\n",
        "  ('free', 'JJ'),\n",
        "  ('shipping', 'NN'),\n",
        "  ('on', 'IN'),\n",
        "  ('orders', 'NNS'),\n",
        "  ('over', 'IN'),\n",
        "  ('$', '$'),\n",
        "  ('99', 'CD'),\n",
        "  ('.', '.')],\n",
        " [('Exclusions', 'NNS'), ('apply', 'RB'), ('.', '.')],\n",
        " [('Expires', 'NNS'), ('on', 'IN'), ('12/19/2013', 'CD'), ('.', '.')]]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hang on\n",
      "=======\n",
      "\n",
      "What did we do here? \n",
      "\n",
      "We performed something known as Part-of-Speech or POS tagging on words here. \n",
      "\n",
      "What is part of speech tagging? \n",
      "\n",
      "The process of classifying words into their parts of speech and labeling them accordingly is known as *part-of-speech tagging*, POS-tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories. The collection of tags used for a particular task is known as a tagset. \n",
      "\n",
      "This is very important in understanding words that make up sentences and training models for information extraction and classification. \n",
      "\n",
      "![POS Tags](http://3.bp.blogspot.com/-IEOkrijtOZY/UbCcnoX7b_I/AAAAAAAAAEU/lVRN_6jHJA0/s1600/tagset.png)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most basic form of information extraction is called *NP-Chunking*. It is a form of *chunking* one of very popular information extraction algorithms."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
      "pprint.pprint(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('the', 'DT'),\n",
        " ('little', 'JJ'),\n",
        " ('yellow', 'JJ'),\n",
        " ('dog', 'NN'),\n",
        " ('barked', 'VBD'),\n",
        " ('at', 'IN'),\n",
        " ('the', 'DT'),\n",
        " ('cat', 'NN')]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* In order to create an NP-chunker, we will first define a chunk grammar, consisting of rules that indicate how sentences should be chunked. In this case, we will define a simple grammar with a single regular-expression rule. \n",
      "* This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). \n",
      "* Using this grammar, we create a chunk parser, and test it on our example sentence. \n",
      "* The result is a tree, which we can either print, or display graphically."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this is our grammar\n",
      "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "result = cp.parse(sentence)\n",
      "pprint.pprint(result)\n",
      "#result.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tree('S', [Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN')]), ('barked', 'VBD'), ('at', 'IN'), Tree('NP', [('the', 'DT'), ('cat', 'NN')])])\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![tag_drawn](http://nltk.org/book/tree_images/ch07-tree-1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Chunk Representation\n",
      "====================\n",
      "\n",
      "There two ways to represent chunks:\n",
      "\n",
      "* Used as Trees #as example above \n",
      "* Tag format:\n",
      "   - most widespread file representation uses IOB tags. In this scheme, each token is tagged with one of three special chunk tags, I (inside), O (outside), or B (begin). \n",
      "   - A token is tagged as B if it marks the beginning of a chunk. \n",
      "   - Subsequent tokens within the chunk are tagged I. \n",
      "   - All other tokens are tagged O. \n",
      "   - The B and I tags are suffixed with the chunk type, e.g. B-NP, I-NP. Of course, it is not necessary to specify a chunk type for tokens that appear outside a chunk, so these are just labeled O. \n",
      "\n",
      "![IOB format](http://nltk.org/images/chunk-tagrep.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training our own chunker\n",
      "========================\n",
      "\n",
      "First describe text file as IOB chunks. \n",
      "\n",
      "Train models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = '''\n",
      "he PRP B-NP\n",
      "accepted VBD B-VP\n",
      "the DT B-NP\n",
      "position NN I-NP\n",
      "of IN B-PP\n",
      "vice NN B-NP\n",
      "chairman NN I-NP\n",
      "of IN B-PP\n",
      "Carlyle NNP B-NP\n",
      "Group NNP I-NP\n",
      ", , O\n",
      "a DT B-NP\n",
      "merchant NN I-NP\n",
      "banking NN I-NP\n",
      "concern NN I-NP\n",
      ". . O'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "'\\nhe PRP B-NP\\naccepted VBD B-VP\\nthe DT B-NP\\nposition NN I-NP\\nof IN B-PP\\nvice NN B-NP\\nchairman NN I-NP\\nof IN B-PP\\nCarlyle NNP B-NP\\nGroup NNP I-NP\\n, , O\\na DT B-NP\\nmerchant NN I-NP\\nbanking NN I-NP\\nconcern NN I-NP\\n. . O'\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![chunked_draw](http://nltk.org/book/tree_images/ch07-tree-2.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the format of the text above to train custom chunkers. \n",
      "\n",
      "Following are snippets of training files from our annoted charms corpus:\n",
      "\n",
      "Offer Extractor Corpus\n",
      "========================\n",
      "\n",
      "```\n",
      "free JJ B-OFFER\n",
      "argan NN O\n",
      "oil NN O\n",
      "full JJ O\n",
      "size NN O\n",
      "hair NN O\n",
      "care NN O\n",
      ", , O\n",
      "hair NN O\n",
      "color NN O\n",
      "or CC O\n",
      "argan NN O\n",
      "heat NN O\n",
      "brush NN O\n",
      "with IN O\n",
      "purchase NN O\n",
      "of IN O\n",
      "argan NN O\n",
      "heat NN O\n",
      "appliance. NNP O\n",
      "  \n",
      "$5 NN B-OFFER\n",
      "off IN I-OFFER\n",
      "each DT O\n",
      "wrangler NN O\n",
      "plaid VBD O\n",
      "westem NN O\n",
      "shirt NN O\n",
      "when WRB O\n",
      "buying VBG O\n",
      "2 CD O\n",
      "or CC O\n",
      "more JJR O\n",
      "  \n",
      "save NN O\n",
      "50-70% CD B-OFFER\n",
      "off IN I-OFFER\n",
      "select JJ O\n",
      "summer NN O\n",
      "clearance NN O\n",
      "apparel NN O\n",
      "and CC O\n",
      "gear JJ O\n",
      "at IN O\n",
      "golite.com. NNP O\n",
      "valid VBD O\n",
      "while IN O\n",
      "supplies NNS O\n",
      "last. NNP O\n",
      "```\n",
      "\n",
      "Product Extractor Corpus\n",
      "========================\n",
      "\n",
      "```\n",
      "Sexy NNP O\n",
      "lace NN B-PRODUCT\n",
      "girl NN B-PRODUCT\n",
      "shorts NNS I-PRODUCT\n",
      "\n",
      "Pure NN O\n",
      "Body NNP B-PRODUCT\n",
      "tank NN B-PRODUCT\n",
      "\n",
      "Favorite NNP O\n",
      "wireless NN B-PRODUCT\n",
      "bra NN B-PRODUCT\n",
      "\n",
      "Pure NN O\n",
      "Body NNP O\n",
      "long-sleeve NNP B-PRODUCT\n",
      "V-neck NNP B-PRODUCT\n",
      "T NNP B-PRODUCT\n",
      "\n",
      "Flocked NNP O\n",
      "dot VBD O\n",
      "wireless NN B-PRODUCT\n",
      "bra NN I-PRODUCT\n",
      "\n",
      "Fresh JJ O\n",
      "bloom NN O\n",
      "plunge NN O\n",
      "demi NN B-PRODUCT\n",
      "bra NN B-PRODUCT\n",
      "\n",
      "Tea NNP O\n",
      "rose VBD O\n",
      "lace NN B-PRODUCT\n",
      "bow NN O\n",
      "balconette NN B-PRODUCT\n",
      "bra NN I-PRODUCT\n",
      "\n",
      "Flocked NNP O\n",
      "dot VBD O\n",
      "modal JJ B-PRODUCT\n",
      "girl NN I-PRODUCT\n",
      "shorts NNS B-PRODUCT\n",
      "\n",
      "High JJ O\n",
      "impact NN O\n",
      "heathered VBD O\n",
      "sports NNS B-PRODUCT\n",
      "bra NN I-PRODUCT\n",
      "\n",
      "Shirred NNP O\n",
      "floral JJ O\n",
      "hipster NN O\n",
      "tanga NN B-PRODUCT\n",
      "\n",
      "Rose NNP O\n",
      "lace NN O\n",
      "convertible JJ O\n",
      "wireless NN B-PRODUCT\n",
      "bralette NN I-PRODUCT\n",
      "```\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Feature Extraction Code\n",
      "=======================\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def npchunk_features(sentence, i, history):\n",
      "    word, pos = sentence[i]\n",
      "    if i == 0:\n",
      "        prevword, prevpos = \"<START>\", \"<START>\"\n",
      "    else:\n",
      "        prevword, prevpos = sentence[i-1]\n",
      "    if i == len(sentence)-1:\n",
      "        nextword, nextpos = \"<END>\", \"<END>\"\n",
      "    else:\n",
      "        nextword, nextpos = sentence[i+1]\n",
      "    return {\"pos\": pos,\n",
      "            \"word\": word,\n",
      "            \"prevpos\": prevpos,\n",
      "            \"nextpos\": nextpos,\n",
      "            \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),\n",
      "            \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\n",
      "            \"tags-since-dt\": tags_since_dt(sentence, i)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the above logic to extract features and train a chunker model using Max Entropy Classifier. \n",
      "\n",
      "The result is a chunking model that can extract offer and product from deal texts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import genieml.extractors.charms\n",
      "charms_extractor = genieml.extractors.charms.Charms()\n",
      "pprint.pprint(charms_extractor.extract_charms('get 50% off on Addidas shoes at Shoes.com'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'offers': ['50%'], 'products': ['off', 'shoes']}\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}