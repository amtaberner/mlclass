{
 "metadata": {
  "name": "NLP Lab"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "NLP Session"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Tokens"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The key ingredient to any Information Retrieval/Natural Language Processing exercise, is text! We can get this a couple of ways. We can use precompiled corpa like those that come with nltk..."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import nltk.book",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1:"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " Moby Dick by Herman Melville 1851\ntext2:"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " Sense and Sensibility by Jane Austen 1811\ntext3:"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " The Book of Genesis\ntext4:"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " Inaugural Address Corpus\ntext5:"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7:"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " Wall Street Journal\ntext8: Personals Corpus\ntext9:"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " The Man Who Was Thursday by G . K . Chesterton 1908\n"
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "words = list(nltk.book.text1)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "len(words)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": "260819"
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "One thing to note, is that the order of tokens can matter, which is why Python lists are great ways to store tokens for processing. \n"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The other way to get text, is to use a real source, and for folks like us, in the Deal Science space, nothing beats deals! So let's use a real data set, you can download a compilation of Commission Junction deal descriptions from here: https://raw.github.com/dlemphers-rmn/mlclass/master/deals.txt"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import requests",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "deals = requests.get('https://raw.github.com/dlemphers-rmn/mlclass/master/deals.txt').text",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Let's check a small sample to make sure we got something back..."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print deals[1:100]",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "nline Fingerstyle Guitar Lessons\nOnline Country Guitar Lessons\nOnline Rock Guitar Lessons\nEspresso \n"
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Let\u2019s get it into a list so we can do some operations on it."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "deals = deals.split('\\n')\nlen(deals)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 13,
       "text": "59648"
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "And then, let\u2019s grab one to experiment with."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "test_deal = deals[5]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print test_deal",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Florida Vacation Deals- Save up to 50% to Miami, Orlando, Daytona and more top destinations! Starting from only $61/night. Shop Now!\n"
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Let's do our first task, to get this into a list of tokens."
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Tokenization"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "So why do we tokenize? It helps to glean insights from data that is otherwise too dense to parse as large bodies of text. Tokenization is really just the process of breaking up a piece of text into smaller pieces that we can perform some operation on. During this process, we can start to extract the wheat from the chaff, discarding punctuation, and meaningless tokens.\n\nWe can tokenize a number of ways, including naive methods like:\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "test_deal.split(' ')",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 17,
       "text": "[u'Florida',\n u'Vacation',\n u'Deals-',\n u'Save',\n u'up',\n u'to',\n u'50%',\n u'to',\n u'Miami,',\n u'Orlando,',\n u'Daytona',\n u'and',\n u'more',\n u'top',\n u'destinations!',\n u'Starting',\n u'from',\n u'only',\n u'$61/night.',\n u'Shop',\n u'Now!']"
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "But why reinvent the wheel? The nltk package comes with a namespace for tokenization:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import nltk.tokenize",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "This contains implementations of a range of tokenizers, for both sentence tokenization and word tokenization. \n\nOne of the most popular sentence tokenizers is the Punkt tokenizer (an unsupervised tokenizer that use an internal model to find sentence boundaries). Let\u2019s see what that does.\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import nltk.tokenize.punkt\n\npst = nltk.tokenize.punkt.PunktSentenceTokenizer()\n\npst.tokenize(test_deal)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 19,
       "text": "[u'Florida Vacation Deals- Save up to 50% to Miami, Orlando, Daytona and more top destinations!',\n u'Starting from only $61/night.',\n u'Shop Now!']"
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Let\u2019s compare that with a seemingly appropriate naive approach:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "test_deal.split('.')",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 20,
       "text": "[u'Florida Vacation Deals- Save up to 50% to Miami, Orlando, Daytona and more top destinations! Starting from only $61/night',\n u' Shop Now!']"
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "A couple of observations:\n\t\n- We've lost the actual boundary. The exclamation marks and full stop have gone. We may need this detail at some point, for example, in sentiment analysis. \n\n- It didn't detect the exclamation mark boundary.\n\nOK, so now we have some sentences."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "sents = pst.tokenize(test_deal)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Let\u2019s grab one and do something with it.\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "test_sent = sents[0]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "test_sent",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 33,
       "text": "u'Florida Vacation Deals- Save up to 50% to Miami, Orlando, Daytona and more top destinations!'"
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "So, the tokenize package contains a number of word tokenizers, but let\u2019s go with the popular Treebank tokenizer:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "tb = nltk.tokenize.TreebankWordTokenizer()\ntb.tokenize(test_sent)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 24,
       "text": "[u'Florida',\n u'Vacation',\n u'Deals-',\n u'Save',\n u'up',\n u'to',\n u'50',\n u'%',\n u'to',\n u'Miami',\n u',',\n u'Orlando',\n u',',\n u'Daytona',\n u'and',\n u'more',\n u'top',\n u'destinations',\n u'!']"
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Cool, so now our sentence is a collection of tokens. Are we happy with the tokenization?\n\nI don\u2019t like that the hyphen has not been detected. Let\u2019s try another tokenizer."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "wpt = nltk.tokenize.WordPunctTokenizer()\ntoks = wpt.tokenize(test_sent)\ntoks",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 31,
       "text": "[u'Florida',\n u'Vacation',\n u'Deals',\n u'-',\n u'Save',\n u'up',\n u'to',\n u'50',\n u'%',\n u'to',\n u'Miami',\n u',',\n u'Orlando',\n u',',\n u'Daytona',\n u'and',\n u'more',\n u'top',\n u'destinations',\n u'!']"
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Ah, that\u2019s a bit nicer. The key here, is that in ML and NLP, there are no \u201cright\u201d solutions or algorithms, there is only the one that meets your requirement. \n\nOK, so now we have our tokens, but there is a bit of noise in the signal. Let\u2019s see if we can refine the set of tokens.\n\nLet\u2019s start with cleaning out stopwords. Stopwords are really just frequently occuring words that have no material meaning to a topic or IR task. NLTK contains a collection of language specific stopwords, so let\u2019s use them to filter our tokens."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "toks = [tok for tok in toks if tok not in nltk.corpus.stopwords.words()]\ntoks",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 32,
       "text": "[u'Florida',\n u'Vacation',\n u'Deals',\n u'-',\n u'Save',\n u'50',\n u'%',\n u'Miami',\n u',',\n u'Orlando',\n u',',\n u'Daytona',\n u'top',\n u'destinations',\n u'!']"
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "You will find that in IR/NLP, this kind of approach is very standard. Getting to a succinct list of tokens is essential to performing more meaningful operations on text. By the way, you are not constrained by using the stopword set in NLTK for filtering, many times, you will augment the stopword set using static exclusion tokens. \n\nSo we\u2019re getting closer to the essence of our deal. Let\u2019s continue the processing. The next step is to identify the minimum meaningful token length. For our purposes, let\u2019s decide that anything less than 1 character is not meaningful to us."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "toks = [tok for tok in toks if len(tok) > 1]\ntoks",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 34,
       "text": "[u'Florida',\n u'Vacation',\n u'Deals',\n u'Save',\n u'50',\n u'Miami',\n u'Orlando',\n u'Daytona',\n u'top',\n u'destinations']"
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "OK, so now we\u2019re really close. I\u2019ve noticed that there is a number in the tokens. In deals, you will notice that there are a lot of number combinations, and for most tasks, they are meaningless, unless you\u2019re extracting structured data like prices or dates, so for our purpose, let\u2019s remove them. Let\u2019s flex some regex muscle and get rid of it that way."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import re\ntoks = [tok for tok in toks if not re.search('\\d+', tok)]\ntoks",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 35,
       "text": "[u'Florida',\n u'Vacation',\n u'Deals',\n u'Save',\n u'Miami',\n u'Orlando',\n u'Daytona',\n u'top',\n u'destinations']"
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Very good, we now have the essence. The most important pieces of the deal. Last thing to do is normalize the tokens, so we don\u2019t run into any case issues."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "toks = [tok.lower() for tok in toks]\ntoks",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 36,
       "text": "[u'florida',\n u'vacation',\n u'deals',\n u'save',\n u'miami',\n u'orlando',\n u'daytona',\n u'top',\n u'destinations']"
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Frequency"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "This is awesome. We now have a set of tokens for our deal, and already, we can see that there are some interesting tokens. Vacation, florida, orlando. But there are also other tokens that have made their way into our set, deals, save, are these relevant?\n\nIt\u2019s hard to know, really, it depends on what they mean. When we think about unsupervised methods, methods in which we don\u2019t train the system, we need to rely on patterns more and more. \n\nLet\u2019s park this for a second, and go down a different path. We\u2019ll come back to it.\n\nLet\u2019s go back to our original corpus, all our deal descriptions.\n\nSo, now that we understand the general playbook for transforming a corpus into a set of tokens, let\u2019s look at the forest instead of the trees.\n\nLet\u2019s take our original corpus, the deal file, and look at the structure of the terms. In IR/NLP, we use terms and tokens interchangeably, and what we\u2019re talking about are the smallest unit of information. \n\nI want to introduce everyone to a handy module in NLTK, called the reader package. We\u2019re going to use a plaintext reader for this next exercise, but I encourage everyone to explore the reader package to see what other interesting readers there are, such as the XMLCorpusReader.\n\nReaders simply encapsulate all the heavy lifting we did before, to process a set of text files using configurable sentence and word tokenizers, amongst other settings. The default sentence and word tokenizers are Punkt, so we\u2019ll roll with that. OK, so let\u2019s load up our file again:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import nltk.corpus.reader.plaintext",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "wordlist = nltk.corpus.reader.plaintext.PlaintextCorpusReader('/home/david/Downloads', ['deals.txt'])",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "How many words do we have in our deal file?"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "len(wordlist.words())",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 40,
       "text": "838772"
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Let\u2019s slice it up and see what\u2019s in there?"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "wordlist.words()[1:30]",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 41,
       "text": "['Fingerstyle',\n 'Guitar',\n 'Lessons',\n 'Online',\n 'Country',\n 'Guitar',\n 'Lessons',\n 'Online',\n 'Rock',\n 'Guitar',\n 'Lessons',\n 'Espresso',\n 'Frames',\n 'Now',\n '40',\n '%',\n 'Off',\n 'from',\n 'PictureFrames',\n '.',\n 'com',\n '!',\n 'Black',\n 'Frames',\n 'Now',\n '40',\n '%',\n 'Off',\n 'from']"
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Interesting\u2026.\n\nLet\u2019s talk about the topic of n-grams. Grams are simply the smallest unit of information. Let\u2019s think of them as grains of rice. Depending on our task, we may want 1 grain of rice, 2 grains, n-grains. Sometimes the grains will be words, sometimes they might be characters. It all depends on your task.\n\nEnough jibber jabber, let\u2019s see what these look like."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "unigrams = [word.lower() for word in wordlist.words()]\nunigrams[1:10]",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 42,
       "text": "['fingerstyle',\n 'guitar',\n 'lessons',\n 'online',\n 'country',\n 'guitar',\n 'lessons',\n 'online',\n 'rock']"
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "These are unigrams, we could process them a little more, using a technique called inflection reduction, where we either try to stem a word (like chopping off the trailing \u2018s) or something more elegant like lemmatization (morphological analysis), but for our purpose, we\u2019re ok with them being a little crunchy. \n\nSo these are all the individual words found in our deals. We can already see some reoccurring, like guitar, and online. \n\nBut there is an interesting pattern in our unigrams, a repeating group, guitar lessons. Maybe these two words keep appearing together for a reason? In IR/NLP, we call this a colocation. And they are meaningful. Also, they appear to be 2 unigrams together, again, we call this a bigram. What other bigrams are out there? Let\u2019s see.\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "bigrams = nltk.bigrams(unigrams)\nbigrams[1:10]",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 43,
       "text": "[('fingerstyle', 'guitar'),\n ('guitar', 'lessons'),\n ('lessons', 'online'),\n ('online', 'country'),\n ('country', 'guitar'),\n ('guitar', 'lessons'),\n ('lessons', 'online'),\n ('online', 'rock'),\n ('rock', 'guitar')]"
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Huh, interesting. But all that happened was we started at the front of our word list, and created a new list of words which had (n<sub>i</sub>,n<sub>i+1</sub>). What about trigrams?"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "trigrams = nltk.trigrams(unigrams)\ntrigrams[1:10]",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 44,
       "text": "[('fingerstyle', 'guitar', 'lessons'),\n ('guitar', 'lessons', 'online'),\n ('lessons', 'online', 'country'),\n ('online', 'country', 'guitar'),\n ('country', 'guitar', 'lessons'),\n ('guitar', 'lessons', 'online'),\n ('lessons', 'online', 'rock'),\n ('online', 'rock', 'guitar'),\n ('rock', 'guitar', 'lessons')]"
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Hmm, again, very interesting. Or not!\n\nOK, so these tokens are cool, but we don\u2019t have a sense of their importance. Let\u2019s assume a basic assumption where the more interesting a word, the more rare it is. Again, NLTK has a cool capability called the FreqDist. Things get more interesting now."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "uni_fd = nltk.FreqDist(unigrams)\n\t\nprint '{} - {}'.format('online', uni_fd['online'])\nprint '{} - {}'.format('guitar', uni_fd['guitar'])",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "online - 1838\nguitar - 74\n"
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import random\n\nfor i in range(1,10):\n    term = random.choice(unigrams)\n    print '[{}] - ({})'.format(term, uni_fd[term])",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[soaring] - (10)\n[supervised] - (2)\n[page] - (2184)\n[in] - (5054)\n[9] - (2751)\n[products] - (1785)\n[handle] - (16)\n[coach] - (24)\n[10] - (2239)\n"
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Ah, now this is very interesting, as we would instinctively think, the token guitar is more meaningful than the term online.\n\nWhat about our collocations?"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "bi_fd = nltk.FreqDist(bigrams)\n\nprint '{} - {}'.format(('guitar', 'lessons'), bi_fd[('guitar', 'lessons')])\nprint '{} - {}'.format(('fingerstyle', 'guitar'), bi_fd[('fingerstyle', 'guitar')])",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "('guitar', 'lessons') - 9\n('fingerstyle', 'guitar') - 3\n"
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Hang on, didn\u2019t we say, the more rare a term, the more meaningful? Well, yes and no. Yes when we\u2019re talking about unigrams, but with bigrams, or any n-gram, it\u2019s the opposite. The more frequent the collocation, the more likely it\u2019s bona fide, and not a random collocation. \n\nFreqDist\u2019s are very cool. For example, we can do complex tasks simply and quickly, such as finding the top 10 bigrams. As always, with NLTK, we can do it the hard way, or the easy way. \n\nLet\u2019s this time, use the nltk.BigramCollocationFinder in NLTK, to do some heavy lifting for us."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import nltk.collocations\n\nbcf = nltk.BigramCollocationFinder.from_words(unigrams)\nbcf.nbest(nltk.collocations.BigramAssocMeasures().pmi, 10)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 55,
       "text": "[(')*', 'crockery'),\n ('+:', 'bgosh20'),\n ('000iu', 'apricots'),\n ('04411gu', '9263024'),\n ('102', 'aw'),\n ('1102', 'nerc'),\n ('122', 'onwards'),\n ('142', '455'),\n ('16uy', '\\\\'),\n ('1967', '1968')]"
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "PMI, or pointwise mutual information is a technique used when measuring collocations, to ensure you don\u2019t fall into the random collocation issue, or the accidental collocation of terms.\n\nSo what do we see?\n\nHmm, that\u2019s not very helpful. What happened? Well, we need to be a little more specific about our needs. Asking for the most frequent bigram is not very helpful, since even a bigram appearing once, with a solid pmi, will top the list. Let\u2019s set a boundary, for example, if it doesn\u2019t appear more than 4 times, we don\u2019t think it\u2019s important."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "bcf.apply_freq_filter(4) \nbcf.nbest(nltk.collocations.BigramAssocMeasures().pmi, 10)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 56,
       "text": "[('+/-', 'rw'),\n ('boxversion', 'erhltlich'),\n ('buenos', 'aires'),\n ('carrie', 'underwood'),\n ('scsi', 'unmap'),\n ('stuart', 'weitzman'),\n ('ultime', 'novit'),\n ('abu', 'dhabi'),\n ('jeannie', 'deva'),\n ('macular', 'degeneration')]"
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Ah, now that\u2019s better. It\u2019s still not perfect, but what we\u2019ve done is another very important part of our IR/NLP journey, we\u2019ve done our first tune. Tuning is critical to getting the most out of machine learning, because if we don\u2019t provide parameters, the machine is unbounded, which introduces anomalies.\n\nOK, you're turn! Do it for Trigrams\u2026"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "OK, so remember we said we would revisit our deal? Let\u2019s do that now."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "toks",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 57,
       "text": "[u'florida',\n u'vacation',\n u'deals',\n u'save',\n u'miami',\n u'orlando',\n u'daytona',\n u'top',\n u'destinations']"
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "So let's work out what the frequency of these terms are"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "for tok in toks:\n    print '{} - {}'.format(tok, uni_fd[tok])",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "florida - 55\nvacation - 265\ndeals - 1462\nsave - 4066\nmiami - 57\norlando - 170\ndaytona - 9\ntop - 1050\ndestinations - 207\n"
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "OK, now lets make it into something we can process"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "tok_tf = [(tok, uni_fd[tok]) for tok in toks]\ntok_tf",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 59,
       "text": "[(u'florida', 55),\n (u'vacation', 265),\n (u'deals', 1462),\n (u'save', 4066),\n (u'miami', 57),\n (u'orlando', 170),\n (u'daytona', 9),\n (u'top', 1050),\n (u'destinations', 207)]"
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Excellent! Now, let's sort it to see the most important tokens"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "sorted(tok_tf, key=lambda tok_freq: tok_freq[1])",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 61,
       "text": "[(u'daytona', 9),\n (u'florida', 55),\n (u'miami', 57),\n (u'orlando', 170),\n (u'destinations', 207),\n (u'vacation', 265),\n (u'top', 1050),\n (u'deals', 1462),\n (u'save', 4066)]"
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "And, voila! As we would expect, a deal about vacations, should contain important information such as the destinations :)"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "The End"
    }
   ],
   "metadata": {}
  }
 ]
}